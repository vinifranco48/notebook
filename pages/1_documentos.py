import streamlit as st
from pathlib import Path
import asyncio
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_groq import ChatGroq
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import os

# Configura√ß√£o da p√°gina Streamlit
st.set_page_config(
    page_title="Chat com Documentos",
    page_icon="üìö",
    layout="centered"
)

# Estiliza√ß√£o CSS personalizada
st.markdown("""
    <style>
    .stTextInput {
        padding: 10px;
    }
    .chat-message {
        padding: 1rem;
        border-radius: 0.5rem;
        margin-bottom: 1rem;
        display: flex;
        flex-direction: column;
    }
    .user-message {
        background-color: #e6f3ff;
    }
    .assistant-message {
        background-color: #f0f2f6;
    }
    </style>
""", unsafe_allow_html=True)

# Prompt personalizado
CUSTOM_PROMPT = """
Voc√™ √© um tutor especializado em criar experi√™ncias de aprendizado personalizadas a partir de documentos. Sua miss√£o √© transformar o conte√∫do t√©cnico em explica√ß√µes claras e envolventes.

Contexto do Documento:
{context}

Pergunta do Usu√°rio:
{question}

Instru√ß√µes de Resposta:


1. Explica√ß√£o Did√°tica
- Explique cada conceito usando linguagem clara e acess√≠vel
- Forne√ßa exemplos pr√°ticos quando poss√≠vel
- Use analogias para tornar conceitos complexos mais compreens√≠veis
- Inclua dicas e observa√ß√µes importantes
- Seja didatico porem direto.

Por favor, mantenha um tom amig√°vel e encorajador, adequado para aprendizado.

Resposta:
"""

@st.cache_resource
def initialize_rag_system():
    """Inicializa o sistema RAG com cache para evitar recarregamento"""
    # Configurar Groq API key
    os.environ["GROQ_API_KEY"] = "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"
    
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-mpnet-base-v2"
    )
    
    # Carregar o √≠ndice FAISS
    if not Path("faiss_index").exists():
        st.error("√çndice FAISS n√£o encontrado. Por favor, execute primeiro o script de processamento dos PDFs.")
        st.stop()
    
    vectorstore = FAISS.load_local(
        "faiss_index",
        embeddings,
        allow_dangerous_deserialization=True
    )
    
    # Criar o template do prompt
    prompt_template = PromptTemplate(
        template=CUSTOM_PROMPT,
        input_variables=["context", "question"]
    )
    
    # Configurar o modelo LLM
    llm = ChatGroq(
        temperature=0.1,
        model_name="llama3-70b-8192",
        max_tokens=1024
    )
    
    # Criar a chain de QA
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(search_kwargs={"k": 4}),
        chain_type_kwargs={
            "prompt": prompt_template,
            "verbose": False
        }
    )
    
    return qa_chain

async def get_response(qa_chain, query):
    """Fun√ß√£o ass√≠ncrona para obter resposta do sistema RAG"""
    response = await qa_chain.ainvoke({"query": query})
    return response['result']

def main():
    st.title("üí¨ Chat com Documentos")
    st.subheader("Fa√ßa perguntas sobre seus documentos")
    
    # Inicializar o hist√≥rico de chat na sess√£o se n√£o existir
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    
    # Inicializar o sistema RAG
    qa_chain = initialize_rag_system()
    
    # Mostrar mensagens do hist√≥rico
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # Input do usu√°rio
    if prompt := st.chat_input("Digite sua pergunta aqui"):
        # Adicionar mensagem do usu√°rio ao hist√≥rico
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Gerar e mostrar resposta do assistente
        with st.chat_message("assistant"):
            with st.spinner("Pensando..."):
                response = asyncio.run(get_response(qa_chain, prompt))
                st.markdown(response)
                # Adicionar resposta ao hist√≥rico
                st.session_state.messages.append({"role": "assistant", "content": response})

    # Bot√£o para limpar o hist√≥rico
    if st.sidebar.button("Limpar Conversa"):
        st.session_state.messages = []
        st.rerun()

if __name__ == "__main__":
    main()